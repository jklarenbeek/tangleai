volumes:
  ollama_storage:
  redis_storage:
  backend_dbstore:

networks:
  demo-docker:
    external: false

x-ollama: &service-ollama
  image: ollama/ollama:latest
  container_name: ollama
  networks: 
    - demo-docker
  pull_policy: always
  tty: true
  restart: always
  environment:
    - OLLAMA_KEEP_ALIVE=24h
    - OLLAMA_HOST=0.0.0.0
  ports:
    - 11434:11434
  volumes:
    - ollama_storage:/root/.ollama

x-init-ollama: &init-ollama
  image: ollama/ollama:latest
  networks: 
    - demo-docker
  container_name: ollama-pull-llama
  volumes:
    - ollama_storage:/root/.ollama
  entrypoint: ["/bin/sh","-c"]
  command: >
    - |
      sleep 3;
      OLLAMA_HOST=ollama:11434 ollama pull llama3.2:3b;
      OLLAMA_HOST=ollama:11434 ollama pull llama3.2:1b;
      OLLAMA_HOST=ollama:11434 ollama pull nomic-embed-text:latest;

services:
  ollama-cpu:
    profiles: ["cpu"]
    <<: *service-ollama

  ollama-nvidia:
    profiles: ["nvidia"]
    <<: *service-ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  ollama-radeon:
    profiles: ["radeon"]
    <<: *service-ollama
    environment:
      - HSA_OVERRIDE_GFX_VERSION=9.0.0

  ollama-pull-llama-cpu:
    profiles: ["cpu"]
    <<: *init-ollama
    depends_on:
      - ollama-cpu

  ollama-pull-llama-nvidia:
    profiles: ["nvidia"]
    <<: *init-ollama
    depends_on:
      - ollama-nvidia

  ollama-pull-llama-radeon:
    profiles: ["radeon"]
    <<: *init-ollama
    environment:
      # gfx90c iGPU
      - HSA_OVERRIDE_GFX_VERSION=9.0.0
    depends_on:
      - ollama-radeon

  redis:
    image: 'docker.dragonflydb.io/dragonflydb/dragonfly'
    ulimits:
      memlock: -1
    restart: always
    ports:
      - "6379:6379"
    networks: 
      - demo-docker
    # For better performance, consider `host` mode instead `port` to avoid docker NAT.
    # `host` mode is NOT currently supported in Swarm Mode.
    # https://docs.docker.com/compose/compose-file/compose-file-v3/#network_mode
    # network_mode: "host"
    volumes:
      - redis_storage:/data

  searxng:
    image: docker.io/searxng/searxng:latest
    environment:
      - SEARXNG_REDIS_URL=redis://redis:6379
    volumes:
      - ./compose/searxng:/etc/searxng:rw
    ports:
      - 8080:8080
    networks: 
      - demo-docker
    depends_on:
      - redis
    restart: unless-stopped
  
  scraper:
    profiles:
      - scraper
      - backend
      - all
    build:
      context: ./app/scraper
      dockerfile: Dockerfile
    security_opt:
      - seccomp=./app/scraper/seccomp-chrome.json
    ports:
      - 3002:3002

  backend:
    profiles:
      - backend
      - all
    build:
      context: ./app/backend
      dockerfile: Dockerfile
    image: backend
    environment:
      - REDIS_URL=redis://redis:6379
      - SEARXNG_API_ENDPOINT=http://searxng:8080
      - OLLAMA_API_ENDPOINT=http://ollama:11434
    depends_on:
      - searxng
      - ollama-cpu
    ports:
      - 3001:3001
    volumes:
      - backend_dbstore:/usr/src/app/data
      - ./app/backend/config.toml:/usr/src/app/config.toml
    networks:
      - demo-docker
    restart: unless-stopped

  frontend:
    profiles:
      - frontend
      - all
    build:
      context: ./app/frontend
      dockerfile: Dockerfile
      args:
        - NEXT_PUBLIC_API_URL=http://localhost:3001/api
        - NEXT_PUBLIC_WS_URL=ws://localhost:3001
    image: frontend
    depends_on:
      - backend
    ports:
      - 3000:3000
    networks:
      - demo-docker
    restart: unless-stopped
