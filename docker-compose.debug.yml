version: '3.8'

volumes:
  ollama_storage:
  redis_storage:
  backend_dbstore:

networks:
  demo-docker:
    external: false

x-ollama: &service-ollama
  image: ollama/ollama:latest
  container_name: ollama
  networks: 
    - demo-docker
  pull_policy: always
  tty: true
  restart: always
  environment:
    - OLLAMA_KEEP_ALIVE=24h
    - OLLAMA_HOST=0.0.0.0
  ports:
    - 11434:11434
  volumes:
    - ollama_storage:/root/.ollama

x-init-ollama: &init-ollama
  image: ollama/ollama:latest
  networks: 
    - demo-docker
  container_name: ollama-pull-llama
  volumes:
    - ollama_storage:/root/.ollama
  entrypoint: /bin/sh
  command:
    - "-c"
    - "sleep 3; OLLAMA_HOST=ollama:11434 ollama pull llama3.2:3b;  OLLAMA_HOST=ollama:11434 ollama pull nemotron-mini:4b; OLLAMA_HOST=ollama:11434 ollama pull nomic-embed-text:latest"

services:
  ollama-cpu:
    profiles: ["cpu"]
    <<: *service-ollama

  ollama-gpu:
    profiles: ["nvidia"]
    <<: *service-ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  ollama-pull-llama-cpu:
    profiles: ["cpu"]
    <<: *init-ollama
    depends_on:
      - ollama-cpu

  ollama-pull-llama-gpu:
    profiles: ["nvidia"]
    <<: *init-ollama
    depends_on:
      - ollama-gpu

  redis:
    image: 'docker.dragonflydb.io/dragonflydb/dragonfly'
    ulimits:
      memlock: -1
    restart: always
    ports:
      - "6379:6379"
    networks: 
      - demo-docker
    # For better performance, consider `host` mode instead `port` to avoid docker NAT.
    # `host` mode is NOT currently supported in Swarm Mode.
    # https://docs.docker.com/compose/compose-file/compose-file-v3/#network_mode
    # network_mode: "host"
    volumes:
      - redis_storage:/data

  searxng:
    image: docker.io/searxng/searxng:latest
    volumes:
      - ./searxng:/etc/searxng:rw
    ports:
      - 8080:8080
    networks: 
      - demo-docker
    depends_on:
      - redis
    restart: unless-stopped
  
  backend:
    profiles:
      - backend
      - all
    build:
      context: .
      dockerfile: backend.dockerfile
    image: backend
    environment:
      - NODE_ENV=development
      - LANGCHAIN_VERBOSE=true
      - SEARXNG_API_ENDPOINT=http://searxng:8080
      - OLLAMA_API_ENDPOINT=http://ollama:11434
    depends_on:
      - searxng
      - ollama-cpu
    ports:
      - 3001:3001
      - 9229:9229
    volumes:
      - backend_dbstore:/usr/src/app/data
      - ./config.toml:/usr/src/app/config.toml
    networks:
      - demo-docker
    restart: unless-stopped
    command: ["yarn", "dev:docker"]

  frontend:
    build:
      context: .
      dockerfile: app.dockerfile
      args:
        - NEXT_PUBLIC_API_URL=http://localhost:3001/api
        - NEXT_PUBLIC_WS_URL=ws://localhost:3001
    image: frontend
    depends_on:
      - backend
    ports:
      - 3000:3000
    networks:
      - demo-docker
    restart: unless-stopped
